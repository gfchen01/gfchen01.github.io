<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: July 30, 2024 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.e5d7adca760216d3b7e28ea434e81f6f.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Guofei Chen"><meta name=description content="The answer is - Gumbel-Softmax!"><link rel=alternate hreflang=en-us href=https://gfchen01.github.io/post/gmm_vae/><link rel=canonical href=https://gfchen01.github.io/post/gmm_vae/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hueadd39559d6741e8934748df32946289_7130_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hueadd39559d6741e8934748df32946289_7130_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="https://gfchen01.github.io/post/gmm_vae/featured.png"><meta property="og:site_name" content="Guofei Chen"><meta property="og:url" content="https://gfchen01.github.io/post/gmm_vae/"><meta property="og:title" content="Improving the Sampling in Gaussian Mixture Varitional Encoder - An Important but Easy to Ignore Step | Guofei Chen"><meta property="og:description" content="The answer is - Gumbel-Softmax!"><meta property="og:image" content="https://gfchen01.github.io/post/gmm_vae/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2022-01-23T00:00:00+00:00"><meta property="article:modified_time" content="2022-01-23T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://gfchen01.github.io/post/gmm_vae/"},"headline":"Improving the Sampling in Gaussian Mixture Varitional Encoder - An Important but Easy to Ignore Step","image":["https://gfchen01.github.io/post/gmm_vae/featured.png"],"datePublished":"2022-01-23T00:00:00Z","dateModified":"2022-01-23T00:00:00Z","author":{"@type":"Person","name":"Guofei Chen"},"publisher":{"@type":"Organization","name":"Guofei Chen","logo":{"@type":"ImageObject","url":"https://gfchen01.github.io/media/icon_hueadd39559d6741e8934748df32946289_7130_192x192_fill_lanczos_center_3.png"}},"description":"The answer is - Gumbel-Softmax!"}</script><title>Improving the Sampling in Gaussian Mixture Varitional Encoder - An Important but Easy to Ignore Step | Guofei Chen</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=d5a720380cdf8110913b2b977fdfb8b9><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Guofei Chen</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Guofei Chen</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/#featured><span>Publications</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Improving the Sampling in Gaussian Mixture Varitional Encoder - An Important but Easy to Ignore Step</h1><p class=page-subtitle>The answer is - Gumbel-Softmax!</p><div class=article-metadata><span class=article-date>Jan 23, 2022</span>
<span class=middot-divider></span>
<span class=article-reading-time>12 min read</span></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:318px;max-height:158px><div style=position:relative><img src=/post/gmm_vae/featured_hu9eced212f46ca36afb1969466432b04c_8184_720x2500_fit_q75_h2_lanczos_3.webp width=318 height=158 alt class=featured-image></div></div><div class=article-container><div class=article-style><h1 id=soft-sensor-using-gaussian-mixture-variational-auto-encoder--embed-unsupervised-category>Soft Sensor Using Gaussian Mixture Variational Auto-Encoder : Embed Unsupervised Category</h1><h2 id=introduction>Introduction</h2><p>VAE is widely used in process industry for a long time. However, these works mostly focus on the nonlinear feature, and the complex dynamic processes with multi-mode characteristics are seldomly considered. Gaussian Mixture VAE(GM-VAE) can capture the multi-mode character of process, but in the GM-VAE put-forward before, an important structure is erroneous (in the view point of Bayesian Inference). I have fixed this issue using Gumbel-Softmax reparameterization and improved the model&rsquo;s performance.</p><p>This work is advised by Prof. Ge at Zhejiang University. I deeply appreciate his help, and you can follow his work on Google Scholar (10k+ citations). <a href="https://scholar.google.com/citations?user=g_EMkuMAAAAJ" target=_blank rel=noopener>link</a></p><p>For those who are familiar with VAE, please directly jump to Part IV: Unsupervised Categories and Multi-mode: GM-VAE. But I strongly recommend that you read the &ldquo;why we use VAE in process industry&rdquo;, to have some insight into our task.</p><h2 id=soft-sensor>Soft Sensor</h2><p>&ldquo;Soft Sensor&rdquo; can be viewed as the contradiction of &ldquo;Hard Sensor&rdquo;. The principle of sensors are based on some physics laws, and modern sensors typically involve circuits. However, there are times when sensor cannot output critical variables at a higher frequency, or online. For example, in process engineering, process variables such as the concentration of product, are critical to controlling the whole process. However, we need to use methods such as liquid chromatography, which will take minutes to finish; or the process is long, and the inspection of a key process variable has a large delay. At the same time, some process variables (the pressure, the temperature) can be measured at a much higher frequency, and has correlation between these variables with those critical variables. This give rise to soft sensor: use data mining to approximate the process model, and infer the critical but hard to measure variables using accessible measurements.</p><p>One may ask: why don&rsquo;t directly derive the model? For many chemical processes, we can&rsquo;t, at least for now. As chemical processes are becoming more and more complicated, using first-principle is untractable in many cases. Instead of &ldquo;deriving&rdquo; models and check them using statistical criteria, some &ldquo;universal&rdquo; (by universal, I mean it can approximate a large family of functions) models that converge to a set of parameters arises &ndash; Machine Learning. This is the topic of the blog.</p><p>**Note that in the article, we use AE and VAE to represent auto-encoder and variational auto-encoder respectively. **</p><h2 id=preliminaries-bayesian-inference-vae-and-why-using-them>Preliminaries: Bayesian Inference, VAE, and why using them</h2><h3 id=ae>AE</h3><p>Variational auto-encoders are based on Bayesian Inference, and originated from auto-encoder, a generative unsupervised model. The idea of auto-encoder is quite simple:</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./Picture/ae.png alt loading=lazy data-zoomable></div></div></figure></p><p>The key to auto-encoder is the bottle-neck in the middle. The dimension in the middle must be smaller than that of two sides, in hope of &ldquo;squeezing&rdquo; the characters from datasets. The design of auto-encoder is intriguing, because it tells us something about &ldquo;generative&rdquo;. Auto-encoder doesn&rsquo;t have good performance, and the structure that brings are much wider use of it are its variants: Stacked Auto-encoder (SAE), Variational Auto-encoder (VAE).</p><h3 id=vae-and-bayesian-inference>VAE and Bayesian Inference</h3><p>VAE regards the latent variables generated by encoder as &ldquo;distributions&rdquo;, and then sample from them. The loss of the entire system is derived using Bayesian Inference.</p><p>Denote the dataset as $D$, the latent variables as $\Omega$. $D$ is generated by $\Omega$. The key idea in Bayesian Inference is that we can estimate how close we are to the truth even if we have no knowledge about the truth. Assume there is a real distribution of the latent variables $\Omega$, which is $p(\Omega|D)$; and we infer a distribution of $\Omega$ using $D$: $q(\Omega)$ (Actually it&rsquo;s $q(\Omega|D)$, but we aren&rsquo;t going to touch the conditional distribution, so we write in a compact form). Now we need to measure the &ldquo;distance&rdquo; between $p(\Omega|D)$ and $q(\Omega)$. A reasonable choice is Kullback–Leibler divergence (KL Divergence in short).
$$
D_{KL}(q(\Omega)||p(\Omega|D)) = E_{q(\Omega)}(log\frac{q(\Omega)}{p(\Omega|D)})\
=\int q(\Omega)log\frac{q(\Omega)p(D)}{p(D|\Omega)p(\Omega)}d\Omega \qquad (bayesian\ rule)\
=D_{KL}(q(\Omega)||p(\Omega)) - \int q(\Omega)log\ p(D|\Omega)d\Omega + log\ p(D)
$$
The last term is a constant. Omit it. Define Evidence Lower Bound (ELBO) :
$$
ELBO :=-D_{KL}(q(\Omega)||p(\Omega)) + \int q(\Omega)log\ p(D|\Omega)d\Omega
$$
By maximizing ELBO, we can minimize the KL Divergence between $q(\Omega)$ and $p(\Omega|D)$. The next question is how to estimate these terms. For simplicity, assume $\Omega$ admits a conditional gaussian distribution given $D$, then the first term can be transformed into a regularization term. You can find the detailed reasoning in appendix. As for the second term, it&rsquo;s impossible to accurately calculate the integral here, but we can sample from the distribution of $q(\Omega)$, as we have access to the distribution because we built it! $p(D|\Omega)$ is more difficult to build, but if you still remember the structure of auto-encoder, we have built $p(\hat{D})$ using $\Omega$, thus if we assume the distribution of $\hat{D}$ admits a gaussian distribution, $p(D|\Omega)$ is estimated as well. Above all, we have estimated the distance between a distribution that we have no access to (the true distribution $p(\Omega|D)$) with something that we build ($q(\Omega)$). Very elegant, isn&rsquo;t it?</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./Picture/VAE_classical.jpg alt loading=lazy data-zoomable></div></div></figure></p><h3 id=why-using-vae-in-process-industry>Why using VAE in process industry?</h3><p>The ability for a model to generate something is so attractive in vision and NLP studies, because it gives the public a sense that the model is &ldquo;alive&rdquo;. In fact, the variants of auto-encoder, like stack auto-encoder and variational auto-encoder, are most widely used for generation tasks for image generation and speech generation, rather than discriminative tasks. But we are going to use it for a discriminative task - regression, in process industry. This is, in my opinion, related to an important ability of VAE: it&rsquo;s tolerance to noise.</p><p>Typically, there are three kinds of hard sensor data being widely used to build a soft sensor, they are: thermometer, pressure gauge, and flow meter. Those who have fine-tuned a chemical process PID controller should be familiar with some principles, one of which is to use only PI instead of PID when flow meter is included in the control cycle. The noise of flow meter is related to the magnitude of flow, the temperature, and they are widely distributed in a wide range of frequencies if you perform FFT to analyze it. Here is an example, Debutanizer column. We will work on datasets drawn from the process later.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./Picture/DC.png alt loading=lazy data-zoomable></div></div></figure></p><p>These are two flow meters&rsquo; sensor readings recorded and later used one dimension of input by our model. As mentioned before, they are noisy and the noise is relevant to many factors.</p><h2 id=unsupervised-categories-and-multi-mode-gm-vae>Unsupervised Categories and Multi-mode: GM-VAE</h2><p>Multi-mode character is common in process industry. Though different types of soft sensor modeling techniques have been applied for quality prediction, most of them are based on the assumption that process data are generated from a single operating region and follow a unimodal Gaussian distribution. For complex multiphase/multimode processes that are running at multiple operating conditions, the basic assumption of multivariate Gaussian distribution does not met because of the mean shifts or covariance changes.</p><p>By taking sufficient linear combinations of single multivariate Gaussian distributions, Gaussian Mixture Model can smoothly approximate any continuous density to arbitrary accuracy. To use Gaussian Mixture in VAE, we have to add an additional category encoder, so the model looks like:</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://yuque.zju.edu.cn/images/yuque/0/2022/png/21858/1645724174929-02023169-d5c0-45c5-806e-d9b7acb2fda0.png alt=image.png loading=lazy data-zoomable></div></div></figure></p><p>The derivation of ELBO is similar. Denote the category as $c$, and labels as $y$. We have:
$$
ELBO = E_{q(\Omega|D, c)q(c|D)}(log\ p(D|\Omega))\+E_{q(\Omega|D, c)q(c|D)}(log\ p(y|\Omega))\
-E_{q(c|D)}(D_{KL}(q(\Omega|D,c)||p(\Omega|c)))\
-D_{KL}(q(c|D) || p(c))
$$
The details of the derivation are in the appendix. Assume the prior categorical distribution is a uniform distribution, and prior latent variable distribution is a zero norm Gaussian distribution, calculate the expectation above (tedious), we have:
$$
Loss = -ELBO=\frac{1}{N}||x-\hat{x}||^2 +\frac{1}{N}||y-\hat{y}||^2+\Sigma\ c_klog\frac{c_k}{c_k^o}+\frac{1}{2}\Sigma\ log\ \sigma^2 + 1 - \sigma^2-\mu^2
$$
The purpose of encoder is to generate latent variables for different categories. For example, when there are 5 categories, and the dimension of latent variables are 10, then encoder will output 5*10 latent variables. The selection network classifies the input and gives the probability of the category. After that, latent variables are sent to different decoders based on their categories. Finally the network <strong>mixes</strong> the decoder output to reconstruct the piece of data and infer the label (At the same time, using a MLP).</p><p>Everything seems fine, but it is worth noticing how the network deals with the output of encoder and selection network. Conventionally, researchers <strong>mix</strong> them by using a Softmax layer after the selection network and give the weighted sum of latent variables to the decoder. It is intuitive, and it works in some cases, so they didn&rsquo;t notice, but it&rsquo;s wrong, because it&rsquo;s against the premise: the model admits Gaussian Mixture.</p><p>For a mixture model, we assume that the input of any instance belongs to exactly one category. The regression rule of different categories can be quite different, and we want the model to learn these different rules. In reality, the labels (in our case, the key process variables) don&rsquo;t come from a mixture of different models; instead, they come from exactly one model. It makes no sense at all to take the weighted sum of decoder output, and by doing that, the different regression rules we hope to estimate converges to the &ldquo;mean&rdquo; of these rules, as none of them could have a big difference to the final outcome when their outputs are mixed. It is confusing, because if we focus on estimating the first two expectation terms, taking a weighted sum seems to be the right way of evaluating the expectation over a categorical distribution.</p><p>Now that it&rsquo;s not correct, how can we estimate the value? Just like a common practice in VAE to sample the continuous distribution generated by encoder, sampling the categorical distribution is a proper choice, and when it comes to sampling, a technique must be used: reparameterization trick. In order to back-propagate, we have to make sure the derivative of the loss to every parameter in the network can be evaluated. Assume $x$ ~ $N(\mu, \sigma)$, we can sample it from a distribution that has the same norm and variance:
$$
x=\mu + \epsilon*\sigma\
$$</p><center>$\epsilon$ ~ $uniform(0, 1)$</center><p>So that the sampling process is differentiable.</p><p>To sample a categorical distribution, Gumbel Softmax is the tool for neural networks. It was proposed by <a href=https://arxiv.org/pdf/1611.01144.pdf%20http://arxiv.org/abs/1611.01144.pdf target=_blank rel=noopener>Eric J, et.al.</a>, 2016. In short, Gumbel Softmax provides a method to draw a sample from a categorical distribution and still keeps model parameters differentiable.</p><p>Training the model with Gumbel Softmax is harder, because it frequently leads to gradient vanish problems. After tuning the cooling rate of Gumbel softmax and many other parameters with care, the model finally worked.</p><h2 id=case-study>Case Study</h2><p>Let&rsquo;s invite our friend &ndash; debutanizer column (DC).</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./Picture/DC.png alt loading=lazy data-zoomable></div></div></figure></p><table><thead><tr><th>Process Variables</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>U1</td><td>$^{\circ}C$</td><td>Top temprature</td></tr><tr><td>U2</td><td>$kg * cm^{-2}$</td><td>Top pressure</td></tr><tr><td>U3</td><td>$m^3 * h^{-1}$</td><td>Reflux flowrate</td></tr><tr><td>U4</td><td>$m^3*h^{-1}$</td><td>Top distillate rate</td></tr><tr><td>U5</td><td>$^{\circ}C$</td><td>Temperature of 9^th^ tray</td></tr><tr><td>U6</td><td>$^{\circ}C$</td><td>Bottom temperature A</td></tr><tr><td>U7</td><td>$^{\circ}C$</td><td>Bottom temperature B</td></tr></tbody></table><p>The figure above presents the flowchart of the debutanizer column. Debutanizer column is an important part of the de-sulfuring and naphtha splitter plant in the refinery. As the debutanizer column is required to maximizing the pentane (C5) content in the overheads distillate and minimize the butane (C4)
content in the bottom flow simultaneously, to improve the performance of the quality control, it is
necessary to carry out the real time prediction of the butane content at the bottom flow. However, the
butane content is not directly on the bottom flow, but on the overheads of the sequential deisopentanizer
column by the gas chromatograph results in a large measuring delay.</p><p>Take the variables listed above, and train the GM-VAE proposed before. Key parameters are listed below:</p><p>Expanded Training Data dimension: 13</p><p>Latent Variable Dimension: 6</p><p>Number of Components: 4 (of Gaussian Mixture)</p><p>Volume of dataset: 2400</p><p>Batch size: 4</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./Picture/GM_VAE.png alt="GM VAE" loading=lazy data-zoomable></div></div></figure></p><center>GM-VAE: test set prediction (R2 = 0.997)</center><p>If we still use Softmax to mix different categories, the outcome is:</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./Picture/GMVAE_non_Gumbel.png alt=GMVAE_non_Gumbel loading=lazy data-zoomable></div></div></figure></p><center>GM-VAE but without Gumbel Softmax: test set prediction (R2 = 0.948)<p>In contrast with the VAE without Gaussian Mixture, the R^2^ improved, and <strong>noise</strong> is mitigated.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./Picture/VAE.png alt=VAE loading=lazy data-zoomable></div></div></figure></p><center>Vanilla VAE: test set performance (R2 = 0.92)<p>We have also made a comparision between some other popular AE-based process monitor methods: Variable-Wise Weighted Stack Autoencoder, Stacked Target-related Autoencoder, Gated Stacked Target-related Autoencoder.</p><table><thead><tr><th>Model</th><th>VW-SAE</th><th>STAE</th><th>GSTAE</th><th>Multimode-VAE</th></tr></thead><tbody><tr><td>RMSE</td><td>0.03571</td><td>0.0357</td><td>0.0299</td><td>0.00728</td></tr><tr><td>R2</td><td>0.9585</td><td>0.9518</td><td>0.9704</td><td>0.9985</td></tr></tbody></table><p><a href>code</a></p><h2 id=conclusion-and-call-for-inference-of-gaussian-components>Conclusion and Call for inference of Gaussian components</h2><p>Gumbel softmax did make a difference (thanks to the carelessness of our predecessors). VAE suffers from strong noise in industrial datasets, and multi-mode feature in industrial processes is common. Adding the Gaussian mixture assumption and carefully build a generative mixture model make the prediction better and more practical to be deployed into factories.</p><p>But we are still wondering: how can we estimate the number of Gaussian components, or in other words, the number of modes in the process? It&rsquo;s difficult in practice. I have been working on an automated inference on the number of Gaussian components, by means of <strong>Dirichlet Processes</strong>. But the progress is not inspiring, as the model always failed to converge. I have made some attempts and here is the <a href>code</a>. I am very happy to get any advice!</p><h2 id=bibliography>Bibliography</h2><h4 id=gumbel-softmax>Gumbel Softmax</h4><p>Jang, E., Gu, S., & Poole, B. (2016). Categorical reparameterization with gumbel-softmax. <em>arXiv preprint arXiv:1611.01144</em>.</p><h4 id=reviews>Reviews</h4><p>Kingma, D. P., & Welling, M. (2019). An introduction to variational autoencoders. <em>Foundations and Trends® in Machine Learning</em>, <em>12</em>(4), 307-392.</p><p>Zhiqiang Ge. (2017). Review on data-driven modeling and monitoring for plant-wide industrial processes,
<em>Chemometrics and Intelligent Laboratory Systems</em>, Volume 171</p><h4 id=gaussian-mixture-and-vae>Gaussian Mixture and VAE</h4><p>Shao, W., Ge, Z., Yao, L., & Song, Z. (2019). Bayesian nonlinear Gaussian mixture regression and its application to virtual sensing for multimode industrial processes. <em>IEEE Transactions on Automation Science and Engineering</em>, <em>17</em>(2), 871-885.</p><p>Dilokthanakul, N., Mediano, P. A., Garnelo, M., Lee, M. C., Salimbeni, H., Arulkumaran, K., & Shanahan, M. (2016). Deep unsupervised clustering with gaussian mixture variational autoencoders. arXiv preprint arXiv:1611.02648.</p><p>Yuan, X., Ge, Z., & Song, Z. (2014). Soft sensor model development in multiphase/multimode processes based on Gaussian mixture regression. <em>Chemometrics and Intelligent Laboratory Systems</em>, <em>138</em>, 97-109.</p><h4 id=benchmarks>Benchmarks</h4><p>Yuan, X., Huang, B., Wang, Y., Yang, C., & Gui, W. (2018). Deep learning-based feature representation and its application for soft sensor modeling with variable-wise weighted SAE. <em>IEEE Transactions on Industrial Informatics</em>, <em>14</em>(7), 3235-3243</p><p>Sun, Q., & Ge, Z. (2020). Gated stacked target-related autoencoder: A novel deep feature extraction and layerwise ensemble method for industrial soft sensor application. <em>IEEE Transactions on Cybernetics</em>.</p><h4 id=dirichlet-process>Dirichlet Process</h4><p>Radford M. Neal, Markov Chain Sampling Methods for Dirichlet Process Mixture Models, <em>Journal of Computational and Graphical Statistics</em>, 9:2, 249-265, 2000</p></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fgfchen01.github.io%2Fpost%2Fgmm_vae%2F&amp;text=Improving+the+Sampling+in+Gaussian+Mixture+Varitional+Encoder+-+An+Important+but+Easy+to+Ignore+Step" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fgfchen01.github.io%2Fpost%2Fgmm_vae%2F&amp;t=Improving+the+Sampling+in+Gaussian+Mixture+Varitional+Encoder+-+An+Important+but+Easy+to+Ignore+Step" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Improving%20the%20Sampling%20in%20Gaussian%20Mixture%20Varitional%20Encoder%20-%20An%20Important%20but%20Easy%20to%20Ignore%20Step&amp;body=https%3A%2F%2Fgfchen01.github.io%2Fpost%2Fgmm_vae%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fgfchen01.github.io%2Fpost%2Fgmm_vae%2F&amp;title=Improving+the+Sampling+in+Gaussian+Mixture+Varitional+Encoder+-+An+Important+but+Easy+to+Ignore+Step" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Improving+the+Sampling+in+Gaussian+Mixture+Varitional+Encoder+-+An+Important+but+Easy+to+Ignore+Step%20https%3A%2F%2Fgfchen01.github.io%2Fpost%2Fgmm_vae%2F" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Fgfchen01.github.io%2Fpost%2Fgmm_vae%2F&amp;title=Improving+the+Sampling+in+Gaussian+Mixture+Varitional+Encoder+-+An+Important+but+Easy+to+Ignore+Step" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://gfchen01.github.io/><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_hube4a86c15d27d130f8689485363078c2_678871_270x270_fill_q75_lanczos_center.jpg alt="Guofei Chen"></a><div class=media-body><h5 class=card-title><a href=https://gfchen01.github.io/>Guofei Chen</a></h5><p class=card-text>Interested in the science and engineering of making reliable, trustworthy, and scalable navigation systems for mobile robots/manipulators in the real-world. Part-time auto mechanic.</p><ul class=network-icon aria-hidden=true><li><a href=/guofei@cmu.edu><i class="fas fa-envelope"></i></a></li><li><a href=https://github.com/gfchen01 target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul></div></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2024 Me. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.32ee83730ed883becad04bc5170512cc.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script>
<script src=/en/js/wowchemy.min.91534f6cb18c3621254d412c69186d7c.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>